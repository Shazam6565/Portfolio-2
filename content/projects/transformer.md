---
date: '2024-'
title: 'Transformers Text Completion Model'
github: 'https://github.com/Shazam6565/Shazam-GPT'
external: 'https://drive.google.com/file/d/1bG9DRXf_GaF03aYYBklqiRBzWxEGQmsr/view?usp=sharing'
tech:
  - Python
  - PyTorch
  - TensorFlow
  - C++
company: 'FSU CIS 5930'
showInProjects: false
---

Developed a text completion model using the Transformer architecture, focusing on optimising learning rates and hyper-parameters to minimise validation loss and perplexity scores as shown in the
famous research paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762), concepts of which are the core foundation of LLMs like GPT-3, BERT, and BART.
